{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"API/base/","text":"Base Here we define the abstract supertypes that all outlier detectors share as well as the necessary fit and transform methods that have to be implemented for each detector. Types Detector # OutlierDetection.Detector \u2014 Type . Detector :: Union { <: SupervisedDetector , <: UnsupervisedDetector } The union type of all implemented detectors, including supervised, semi-supervised and unsupervised detectors. Note: A semi-supervised detector can be seen as a supervised detector with a specific class representing unlabeled data. source SupervisedDetector # OutlierDetection.SupervisedDetector \u2014 Type . SupervisedDetector This abstract type forms the basis for all implemented supervised outlier detection algorithms. To implement a new SupervisedDetector yourself, you have to implement the fit(detector, X, y)::DetectorModel and transform(detector, model, X)::Scores methods. source UnsupervisedDetector # OutlierDetection.UnsupervisedDetector \u2014 Type . UnsupervisedDetector This abstract type forms the basis for all implemented unsupervised outlier detection algorithms. To implement a new UnsupervisedDetector yourself, you have to implement the fit(detector, X)::DetectorModel and transform(detector, model, X)::Scores methods. source DetectorModel # OutlierDetection.DetectorModel \u2014 Type . DetectorModel A DetectorModel represents the learned behaviour for specific detector. This might include parameters in parametric models or other repesentations of the learned data in nonparametric models. In essence, it includes everything required to transform an instance to an outlier score. source Scores # OutlierDetection.Scores \u2014 Type . Scores :: AbstractVector { <: Real } Scores are continuous values, where the range depends on the specific detector yielding the scores. Note: All detectors return increasing scores and higher scores are associated with higher outlierness. Concretely, scores are defined as an AbstractVector{<:Real} . source Labels # OutlierDetection.Labels \u2014 Type . Labels :: AbstractArray { <: Integer } Labels are used for supervision and evaluation and are defined as an AbstractArray{<:Integer} . The convention for labels is that -1 indicates outliers, 1 indicates inliers and 0 indicates unlabeled data in semi-supervised tasks. source Data # OutlierDetection.Data \u2014 Type . Data :: AbstractArray { <: Real } The raw input data for every detector is defined as AbstractArray{<:Real} and should be a column-major n-dimensional array. The input data used to fit a Detector and transform Data . source Functions fit # OutlierDetection.fit \u2014 Function . fit ( detector , X , y ) Fit a specified unsupervised, supervised or semi-supervised outlier detector. That is, learn a DetectorModel from input data X and, in the supervised and semi-supervised setting, labels y . In a supervised setting, the label -1 represents outliers and 1 inliers. In a semi-supervised setting, the label 0 additionally represents unlabeled data. Note: Unsupervised detectors can be fitted without specifying y , otherwise y is simply ignore. Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. X::Union{AbstractMatrix, Tables.jl-compatible} Either a column-major matrix or a row-major Tables.jl-compatible source. Returns model::DetectorModel The learned model of the given detector, which contains all the necessary information for later prediction. scores::Scores The achieved outlier scores of the given training data X . Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) source transform # OutlierDetection.transform \u2014 Function . transform ( detector , model , X ) Transform input data X to outlier scores using an UnsupervisedDetector or SupervisedDetector and a corresponding DetectorModel . Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. model::DetectorModel The model learned from using fit with a supervised or unsupervised Detector X::Union{AbstractMatrix, Tables.jl-compatible} Either a column-major matrix or a row-major Tables.jl-compatible source. Returns scores::Scores The achieved outlier scores of the given test data X . Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) source","title":"Base"},{"location":"API/base/#base","text":"Here we define the abstract supertypes that all outlier detectors share as well as the necessary fit and transform methods that have to be implemented for each detector.","title":"Base"},{"location":"API/base/#types","text":"","title":"Types"},{"location":"API/base/#detector","text":"# OutlierDetection.Detector \u2014 Type . Detector :: Union { <: SupervisedDetector , <: UnsupervisedDetector } The union type of all implemented detectors, including supervised, semi-supervised and unsupervised detectors. Note: A semi-supervised detector can be seen as a supervised detector with a specific class representing unlabeled data. source","title":"Detector"},{"location":"API/base/#superviseddetector","text":"# OutlierDetection.SupervisedDetector \u2014 Type . SupervisedDetector This abstract type forms the basis for all implemented supervised outlier detection algorithms. To implement a new SupervisedDetector yourself, you have to implement the fit(detector, X, y)::DetectorModel and transform(detector, model, X)::Scores methods. source","title":"SupervisedDetector"},{"location":"API/base/#unsuperviseddetector","text":"# OutlierDetection.UnsupervisedDetector \u2014 Type . UnsupervisedDetector This abstract type forms the basis for all implemented unsupervised outlier detection algorithms. To implement a new UnsupervisedDetector yourself, you have to implement the fit(detector, X)::DetectorModel and transform(detector, model, X)::Scores methods. source","title":"UnsupervisedDetector"},{"location":"API/base/#detectormodel","text":"# OutlierDetection.DetectorModel \u2014 Type . DetectorModel A DetectorModel represents the learned behaviour for specific detector. This might include parameters in parametric models or other repesentations of the learned data in nonparametric models. In essence, it includes everything required to transform an instance to an outlier score. source","title":"DetectorModel"},{"location":"API/base/#scores","text":"# OutlierDetection.Scores \u2014 Type . Scores :: AbstractVector { <: Real } Scores are continuous values, where the range depends on the specific detector yielding the scores. Note: All detectors return increasing scores and higher scores are associated with higher outlierness. Concretely, scores are defined as an AbstractVector{<:Real} . source","title":"Scores"},{"location":"API/base/#labels","text":"# OutlierDetection.Labels \u2014 Type . Labels :: AbstractArray { <: Integer } Labels are used for supervision and evaluation and are defined as an AbstractArray{<:Integer} . The convention for labels is that -1 indicates outliers, 1 indicates inliers and 0 indicates unlabeled data in semi-supervised tasks. source","title":"Labels"},{"location":"API/base/#data","text":"# OutlierDetection.Data \u2014 Type . Data :: AbstractArray { <: Real } The raw input data for every detector is defined as AbstractArray{<:Real} and should be a column-major n-dimensional array. The input data used to fit a Detector and transform Data . source","title":"Data"},{"location":"API/base/#functions","text":"","title":"Functions"},{"location":"API/base/#fit","text":"# OutlierDetection.fit \u2014 Function . fit ( detector , X , y ) Fit a specified unsupervised, supervised or semi-supervised outlier detector. That is, learn a DetectorModel from input data X and, in the supervised and semi-supervised setting, labels y . In a supervised setting, the label -1 represents outliers and 1 inliers. In a semi-supervised setting, the label 0 additionally represents unlabeled data. Note: Unsupervised detectors can be fitted without specifying y , otherwise y is simply ignore. Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. X::Union{AbstractMatrix, Tables.jl-compatible} Either a column-major matrix or a row-major Tables.jl-compatible source. Returns model::DetectorModel The learned model of the given detector, which contains all the necessary information for later prediction. scores::Scores The achieved outlier scores of the given training data X . Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) source","title":"fit"},{"location":"API/base/#transform","text":"# OutlierDetection.transform \u2014 Function . transform ( detector , model , X ) Transform input data X to outlier scores using an UnsupervisedDetector or SupervisedDetector and a corresponding DetectorModel . Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. model::DetectorModel The model learned from using fit with a supervised or unsupervised Detector X::Union{AbstractMatrix, Tables.jl-compatible} Either a column-major matrix or a row-major Tables.jl-compatible source. Returns scores::Scores The achieved outlier scores of the given test data X . Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) source","title":"transform"},{"location":"API/evaluation/","text":"Evaluation TODO","title":"Evaluation"},{"location":"API/evaluation/#evaluation","text":"TODO","title":"Evaluation"},{"location":"API/models/","text":"Models All models, both supervised and unsupervised, define a Detector , which is just a mutable collection of hyperparameters. Each detector implements a fit and transform method, where fitting refers to learning a model from training data and transform refers to using a learned model to calculate outlier scores of test data . Proximity Models ABOD # OutlierDetection.ABOD \u2014 Type . ABOD ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false , enhanced = false ) Determine outliers based on the angles to its nearest neighbors. This implements the FastABOD variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. Notice: The scores are inverted, to conform to our notion that higher scores describe higher outlierness. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. enhanced::Bool When enhanced=true , it uses the enhanced ABOD (EABOD) adaptation proposed by [2]. Examples using OutlierDetection : ABOD , fit , transform detector = ABOD () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data. [2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships. source COF # OutlierDetection.COF \u2014 Type . COF ( k = 5 metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Local outlier density based on chaining distance between graphs of neighbors, as described in [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : COF , fit , transform detector = COF () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns. source DNN # OutlierDetection.DNN \u2014 Type . DNN ( metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Anomaly score based on the number of neighbors in a hypersphere of radius d . Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. d::Real The hypersphere radius used to calculate the global density of an instance. Examples using OutlierDetection : DNN , fit , transform detector = DNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets. source KNN # OutlierDetection.KNN \u2014 Type . KNN ( k = 5 , metric = Euclidean , algorithm =: kdtree , leafsize = 10 , reorder = true , reduction =: maximum ) Calculate the anomaly score of an instance based on the distance to its k-nearest neighbors. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. reduction::Symbol One of (:maximum, :median, :mean) . ( reduction=:maximum ) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability. Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets. [2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces. source LOF # OutlierDetection.LOF \u2014 Type . LOF ( k = 5 metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Calculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : LOF , fit , transform detector = LOF () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, J\u00f6rg (2000): LOF: Identifying Density-Based Local Outliers. source Neural Models AE # OutlierDetection.AE \u2014 Type . AE ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), loss = mse ) Calculate the anomaly score of an instance based on the reconstruction loss of an autoencoder, see [1] for an explanation of auto encoders. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. Examples using OutlierDetection : AE , fit , transform detector = AE () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Aggarwal, Charu C. (2017): Outlier Analysis. source DeepSAD # OutlierDetection.DeepSAD \u2014 Type . DeepSAD ( encoder = Chain () decoder = Chain () batchsize = 32 epochs = 1 shuffle = true partial = false opt = ADAM () loss = mse eta = 1 eps = 1e-6 callback = _ -> () -> ()) Deep Semi-Supervised Anomaly detection technique based on the distance to a hypersphere center as described in [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. eta::Real Weighting parameter for the labeled data; i.e. higher values of eta assign higher weight to labeled data in the svdd loss function. For a sensitivity analysis of this parameter, see [1]. eps::Real Because the inverse distance used in the svdd loss can lead to division by zero, the parameters eps is added for numerical stability. callback::Function Experimental parameter that might change . A function to be called after the model parameters have been updated that can call Flux's callback helpers, see https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1 . Notice: The parameters batchsize , epochs , shuffle , partial , opt and callback can also be tuples of size 2, specifying the corresponding values for (1) pretraining and (2) training; otherwise the same values are used for pretraining and training. Examples using OutlierDetection : DeepSAD , fit , transform detector = DeepSAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model , scores = fit ( detector , X , y ) transform ( detector , model , X ) References [1] Ruff, Lukas; Vandermeulen, Robert A.; G\u00f6rnitz, Nico; Binder, Alexander; M\u00fcller, Emmanuel; M\u00fcller, Klaus-Robert; Kloft, Marius (2019): Deep Semi-Supervised Anomaly Detection. source ESAD # OutlierDetection.ESAD \u2014 Type . ESAD ( encoder = Chain () decoder = Chain () batchsize = 32 epochs = 1 shuffle = false partial = true opt = ADAM () \u03bb1 = 1 \u03bb2 = 1 noise = identity ) End-to-End semi-supervised anomaly detection algorithm similar to DeepSAD, but without the pretraining phase. The algorithm was published by Huang et al., see [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. \u03bb1::Real Weighting parameter of the norm loss, which minimizes the empirical variance and thus minimizes entropy. \u03bb2::Real Weighting parameter of the assistent loss function to define the consistency between the two encoders. noise::Function (AbstractArray{T} -> AbstractArray{T}) A function to be applied to a batch of input data to add noise, see [1] for an explanation. Examples using OutlierDetection : ESAD , fit , transform detector = ESAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model , scores = fit ( detector , X , y ) transform ( detector , model , X ) References [1] Huang, Chaoqin; Ye, Fei; Zhang, Ya; Wang, Yan-Feng; Tian, Qi (2020): ESAD: End-to-end Deep Semi-supervised Anomaly Detection. source","title":"Models"},{"location":"API/models/#models","text":"All models, both supervised and unsupervised, define a Detector , which is just a mutable collection of hyperparameters. Each detector implements a fit and transform method, where fitting refers to learning a model from training data and transform refers to using a learned model to calculate outlier scores of test data .","title":"Models"},{"location":"API/models/#proximity-models","text":"","title":"Proximity Models"},{"location":"API/models/#abod","text":"# OutlierDetection.ABOD \u2014 Type . ABOD ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false , enhanced = false ) Determine outliers based on the angles to its nearest neighbors. This implements the FastABOD variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. Notice: The scores are inverted, to conform to our notion that higher scores describe higher outlierness. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. enhanced::Bool When enhanced=true , it uses the enhanced ABOD (EABOD) adaptation proposed by [2]. Examples using OutlierDetection : ABOD , fit , transform detector = ABOD () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data. [2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships. source","title":"ABOD"},{"location":"API/models/#cof","text":"# OutlierDetection.COF \u2014 Type . COF ( k = 5 metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Local outlier density based on chaining distance between graphs of neighbors, as described in [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : COF , fit , transform detector = COF () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns. source","title":"COF"},{"location":"API/models/#dnn","text":"# OutlierDetection.DNN \u2014 Type . DNN ( metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Anomaly score based on the number of neighbors in a hypersphere of radius d . Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. d::Real The hypersphere radius used to calculate the global density of an instance. Examples using OutlierDetection : DNN , fit , transform detector = DNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets. source","title":"DNN"},{"location":"API/models/#knn","text":"# OutlierDetection.KNN \u2014 Type . KNN ( k = 5 , metric = Euclidean , algorithm =: kdtree , leafsize = 10 , reorder = true , reduction =: maximum ) Calculate the anomaly score of an instance based on the distance to its k-nearest neighbors. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. reduction::Symbol One of (:maximum, :median, :mean) . ( reduction=:maximum ) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability. Examples using OutlierDetection : KNN , fit , transform detector = KNN () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets. [2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces. source","title":"KNN"},{"location":"API/models/#lof","text":"# OutlierDetection.LOF \u2014 Type . LOF ( k = 5 metric = Euclidean () algorithm = : kdtree leafsize = 10 reorder = true parallel = false ) Calculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :brutetree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize transform and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : LOF , fit , transform detector = LOF () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, J\u00f6rg (2000): LOF: Identifying Density-Based Local Outliers. source","title":"LOF"},{"location":"API/models/#neural-models","text":"","title":"Neural Models"},{"location":"API/models/#ae","text":"# OutlierDetection.AE \u2014 Type . AE ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), loss = mse ) Calculate the anomaly score of an instance based on the reconstruction loss of an autoencoder, see [1] for an explanation of auto encoders. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. Examples using OutlierDetection : AE , fit , transform detector = AE () X = rand ( 10 , 100 ) model , scores = fit ( detector , X ) transform ( detector , model , X ) References [1] Aggarwal, Charu C. (2017): Outlier Analysis. source","title":"AE"},{"location":"API/models/#deepsad","text":"# OutlierDetection.DeepSAD \u2014 Type . DeepSAD ( encoder = Chain () decoder = Chain () batchsize = 32 epochs = 1 shuffle = true partial = false opt = ADAM () loss = mse eta = 1 eps = 1e-6 callback = _ -> () -> ()) Deep Semi-Supervised Anomaly detection technique based on the distance to a hypersphere center as described in [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. eta::Real Weighting parameter for the labeled data; i.e. higher values of eta assign higher weight to labeled data in the svdd loss function. For a sensitivity analysis of this parameter, see [1]. eps::Real Because the inverse distance used in the svdd loss can lead to division by zero, the parameters eps is added for numerical stability. callback::Function Experimental parameter that might change . A function to be called after the model parameters have been updated that can call Flux's callback helpers, see https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1 . Notice: The parameters batchsize , epochs , shuffle , partial , opt and callback can also be tuples of size 2, specifying the corresponding values for (1) pretraining and (2) training; otherwise the same values are used for pretraining and training. Examples using OutlierDetection : DeepSAD , fit , transform detector = DeepSAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model , scores = fit ( detector , X , y ) transform ( detector , model , X ) References [1] Ruff, Lukas; Vandermeulen, Robert A.; G\u00f6rnitz, Nico; Binder, Alexander; M\u00fcller, Emmanuel; M\u00fcller, Klaus-Robert; Kloft, Marius (2019): Deep Semi-Supervised Anomaly Detection. source","title":"DeepSAD"},{"location":"API/models/#esad","text":"# OutlierDetection.ESAD \u2014 Type . ESAD ( encoder = Chain () decoder = Chain () batchsize = 32 epochs = 1 shuffle = false partial = true opt = ADAM () \u03bb1 = 1 \u03bb2 = 1 noise = identity ) End-to-End semi-supervised anomaly detection algorithm similar to DeepSAD, but without the pretraining phase. The algorithm was published by Huang et al., see [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. \u03bb1::Real Weighting parameter of the norm loss, which minimizes the empirical variance and thus minimizes entropy. \u03bb2::Real Weighting parameter of the assistent loss function to define the consistency between the two encoders. noise::Function (AbstractArray{T} -> AbstractArray{T}) A function to be applied to a batch of input data to add noise, see [1] for an explanation. Examples using OutlierDetection : ESAD , fit , transform detector = ESAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model , scores = fit ( detector , X , y ) transform ( detector , model , X ) References [1] Huang, Chaoqin; Ye, Fei; Zhang, Ya; Wang, Yan-Feng; Tian, Qi (2020): ESAD: End-to-end Deep Semi-supervised Anomaly Detection. source","title":"ESAD"},{"location":"documentation/architecture/","text":"Architecture TODO","title":"Architecture"},{"location":"documentation/architecture/#architecture","text":"TODO","title":"Architecture"},{"location":"documentation/benchmark-datasets/","text":"Benchmark Datasets TODO","title":"Benchmark Datasets"},{"location":"documentation/benchmark-datasets/#benchmark-datasets","text":"TODO","title":"Benchmark Datasets"},{"location":"documentation/detector-creation/","text":"Detector Creation TODO","title":"Detector Creation"},{"location":"documentation/detector-creation/#detector-creation","text":"TODO","title":"Detector Creation"},{"location":"documentation/ensemble-learning/","text":"Ensemble Learning TODO","title":"Ensemble Learning"},{"location":"documentation/ensemble-learning/#ensemble-learning","text":"TODO","title":"Ensemble Learning"},{"location":"documentation/guide/","text":"Guide TODO","title":"Guide"},{"location":"documentation/guide/#guide","text":"TODO","title":"Guide"},{"location":"documentation/performance-tips/","text":"Performance Tips All the usual Julia performance tips apply . As always profiling your code is generally a useful way of finding bottlenecks. We provide a basic performance benchmarking toolkit. See benchmark/runbenchmarks.jl for usage instructions.","title":"Performance Tips"},{"location":"documentation/performance-tips/#performance-tips","text":"All the usual Julia performance tips apply . As always profiling your code is generally a useful way of finding bottlenecks. We provide a basic performance benchmarking toolkit. See benchmark/runbenchmarks.jl for usage instructions.","title":"Performance Tips"},{"location":"documentation/using-mlj/","text":"Using MLJ TODO","title":"Using MLJ"},{"location":"documentation/using-mlj/#using-mlj","text":"TODO","title":"Using MLJ"},{"location":"getting-started/contributing/","text":"How to Contribute OutlierDetection.jl is a community-driven project and your help is extremely welcome. If you get stuck, please don't hesitate to chat with us or raise an issue . Take a look at Github's How to Contribute Guide to find out more about what it means to contribute. Note: To avoid duplicating work, it is highly advised that you search through the issue tracker and the PR list . If in doubt about duplicated work, or if you want to work on a non-trivial feature, it\u2019s recommended to first open an issue in the issue tracker to get some feedbacks from core developers. Areas of contribution We value all kinds of contributions - not just code. The following table gives an overview of key contribution areas. Area Description Documentation Improve or add docstrings, glossary terms, the user guide, and the example notebooks Testing Report bugs, improve or add unit tests, conduct field testing on real-world data sets Code Improve or add functionality, fix bugs Mentoring Onboarding and mentoring of new contributors Outreach Organize talks, tutorials or workshops, write blog posts Maintenance Manage and review issues/pull requests API design Design interfaces for detectors and other functionality Reporting bugs We use GitHub issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish to see a feature implemented. It is recommended to check that your issue complies with the following rules before submitting: Verify that your issue is not being currently addressed by other issues or pull requests . Please ensure all code snippets and error messages are formatted in appropriate code blocks. See Creating and highlighting code blocks . Please be specific about what detectors and/or functions are involved and the shape of the data, as appropriate; please include a reproducible code snippet or link to a gist . If an exception is raised, please provide the traceback. The contribution workflow The preferred workflow for contributing to OutlierDetection's repository is to fork the main repository on GitHub, clone, and develop on a new branch. Fork the project repository by clicking on the \\'Fork\\' button near the top right of the page. This creates a copy of the code under your GitHub user account. For more details on how to fork a repository see this guide . Clone your fork of the OutlierDetection.jl repo from your GitHub account to your local disk: git clone git@github.com:USERNAME/OutlierDetection.jl.git cd OutlierDetection.jl Configure and link the remote for your fork to the upstream repository: git remote -v git remote add upstream https://github.com/davnn/OutlierDetection.git Verify the new upstream repository you\\'ve specified for your fork: git remote -v > origin https://github.com/USERNAME/YOUR_FORK.git ( fetch ) > origin https://github.com/YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( fetch ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( push ) Sync the main branch of your fork with the upstream repository: git fetch upstream git checkout main --track origin/main git merge upstream/main Create a new feature branch from the main branch to hold your changes: git checkout main git checkout -b <my-feature-branch> Always use a feature branch. It\\'s good practice to never work on the main branch! Name the feature branch after your contribution. Develop your contribution on your feature branch. Add changed files using git add and then git commit files to record your changes in Git: git add <modified_files> git commit When finished, push the changes to your GitHub account with: git push --set-upstream origin my-feature-branch Follow these instructions to create a pull request from your fork. If your work is still work in progress, you can open a draft pull request. We recommend to open a pull request early, so that other contributors become aware of your work and can give you feedback early on. To add more changes, simply repeat steps 7 - 8. Pull requests are updated automatically if you push new changes to the same branch. If any of the above seems like magic to you, please look up the Git documentation on the web. If you get stuck, feel free to chat with us . Continuous integration We use continuous integration services on GitHub to automatically check if new pull requests do not break anything on all the Julia versions we support. The main quality control measures right now are unit testing and test coverage . In the future we additionally want to check code style and formatting. Unit testing We use Julia's built-in Unit Testing . The tests can be found in the test folder. To check if your code passes all tests make sure that you have the OutlierDetection environment activated and run ] test from the Julia console. Test coverage We use the Coverage.jl package and codecov to measure and compare test coverage of our code. API design We follow the general design approach chosen by MLJ, which is described in the paper \"Designing Machine Learning Toolboxes: Concepts, Principles and Patterns\" . Additionally, we are always looking for feedback and improvement suggestions! Documentation We use Documenter.jl and mkdocs to build and deploy our online documention. The source files used to generate the online documentation can be found in docs/src/ . For example, the main configuration file for mkdocs is mkdocs.yml and the main page is index.md . To add new pages, you need to add a new .md file and include it in the mkdocs.yml file. To build the documentation locally, you need to navigate to docs/ and Build the markdown files with Documenter.jl: julia --project make.jl To build the website using the markdown files, run: mkdocs build # optionally run `mkdocs serve` to build and serve locally You can find the generated files in the OutlierDetection.jl/docs/site/ folder. To view the website, open OutlierDetection.jl/docs/site/index.html with your preferred web browser or use mkdocs serve to start a local documentation server. Coding style We use DocumentFormat.jl as a code formatter. Additionally, we use a maximum line length of 120 characters. Acknowledging contributions We follow the all-contributors specification and recognise various types of contributions. Take a look at our past and current contributors ! If you are a new contributor, please make sure we add you to our list of contributors. All contributions are recorded in .all-contributorsrc .","title":"Contributing"},{"location":"getting-started/contributing/#how-to-contribute","text":"OutlierDetection.jl is a community-driven project and your help is extremely welcome. If you get stuck, please don't hesitate to chat with us or raise an issue . Take a look at Github's How to Contribute Guide to find out more about what it means to contribute. Note: To avoid duplicating work, it is highly advised that you search through the issue tracker and the PR list . If in doubt about duplicated work, or if you want to work on a non-trivial feature, it\u2019s recommended to first open an issue in the issue tracker to get some feedbacks from core developers.","title":"How to Contribute"},{"location":"getting-started/contributing/#areas-of-contribution","text":"We value all kinds of contributions - not just code. The following table gives an overview of key contribution areas. Area Description Documentation Improve or add docstrings, glossary terms, the user guide, and the example notebooks Testing Report bugs, improve or add unit tests, conduct field testing on real-world data sets Code Improve or add functionality, fix bugs Mentoring Onboarding and mentoring of new contributors Outreach Organize talks, tutorials or workshops, write blog posts Maintenance Manage and review issues/pull requests API design Design interfaces for detectors and other functionality","title":"Areas of contribution"},{"location":"getting-started/contributing/#reporting-bugs","text":"We use GitHub issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish to see a feature implemented. It is recommended to check that your issue complies with the following rules before submitting: Verify that your issue is not being currently addressed by other issues or pull requests . Please ensure all code snippets and error messages are formatted in appropriate code blocks. See Creating and highlighting code blocks . Please be specific about what detectors and/or functions are involved and the shape of the data, as appropriate; please include a reproducible code snippet or link to a gist . If an exception is raised, please provide the traceback.","title":"Reporting bugs"},{"location":"getting-started/contributing/#the-contribution-workflow","text":"The preferred workflow for contributing to OutlierDetection's repository is to fork the main repository on GitHub, clone, and develop on a new branch. Fork the project repository by clicking on the \\'Fork\\' button near the top right of the page. This creates a copy of the code under your GitHub user account. For more details on how to fork a repository see this guide . Clone your fork of the OutlierDetection.jl repo from your GitHub account to your local disk: git clone git@github.com:USERNAME/OutlierDetection.jl.git cd OutlierDetection.jl Configure and link the remote for your fork to the upstream repository: git remote -v git remote add upstream https://github.com/davnn/OutlierDetection.git Verify the new upstream repository you\\'ve specified for your fork: git remote -v > origin https://github.com/USERNAME/YOUR_FORK.git ( fetch ) > origin https://github.com/YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( fetch ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( push ) Sync the main branch of your fork with the upstream repository: git fetch upstream git checkout main --track origin/main git merge upstream/main Create a new feature branch from the main branch to hold your changes: git checkout main git checkout -b <my-feature-branch> Always use a feature branch. It\\'s good practice to never work on the main branch! Name the feature branch after your contribution. Develop your contribution on your feature branch. Add changed files using git add and then git commit files to record your changes in Git: git add <modified_files> git commit When finished, push the changes to your GitHub account with: git push --set-upstream origin my-feature-branch Follow these instructions to create a pull request from your fork. If your work is still work in progress, you can open a draft pull request. We recommend to open a pull request early, so that other contributors become aware of your work and can give you feedback early on. To add more changes, simply repeat steps 7 - 8. Pull requests are updated automatically if you push new changes to the same branch. If any of the above seems like magic to you, please look up the Git documentation on the web. If you get stuck, feel free to chat with us .","title":"The contribution workflow"},{"location":"getting-started/contributing/#continuous-integration","text":"We use continuous integration services on GitHub to automatically check if new pull requests do not break anything on all the Julia versions we support. The main quality control measures right now are unit testing and test coverage . In the future we additionally want to check code style and formatting.","title":"Continuous integration"},{"location":"getting-started/contributing/#unit-testing","text":"We use Julia's built-in Unit Testing . The tests can be found in the test folder. To check if your code passes all tests make sure that you have the OutlierDetection environment activated and run ] test from the Julia console.","title":"Unit testing"},{"location":"getting-started/contributing/#test-coverage","text":"We use the Coverage.jl package and codecov to measure and compare test coverage of our code.","title":"Test coverage"},{"location":"getting-started/contributing/#api-design","text":"We follow the general design approach chosen by MLJ, which is described in the paper \"Designing Machine Learning Toolboxes: Concepts, Principles and Patterns\" . Additionally, we are always looking for feedback and improvement suggestions!","title":"API design"},{"location":"getting-started/contributing/#documentation","text":"We use Documenter.jl and mkdocs to build and deploy our online documention. The source files used to generate the online documentation can be found in docs/src/ . For example, the main configuration file for mkdocs is mkdocs.yml and the main page is index.md . To add new pages, you need to add a new .md file and include it in the mkdocs.yml file. To build the documentation locally, you need to navigate to docs/ and Build the markdown files with Documenter.jl: julia --project make.jl To build the website using the markdown files, run: mkdocs build # optionally run `mkdocs serve` to build and serve locally You can find the generated files in the OutlierDetection.jl/docs/site/ folder. To view the website, open OutlierDetection.jl/docs/site/index.html with your preferred web browser or use mkdocs serve to start a local documentation server.","title":"Documentation"},{"location":"getting-started/contributing/#coding-style","text":"We use DocumentFormat.jl as a code formatter. Additionally, we use a maximum line length of 120 characters.","title":"Coding style"},{"location":"getting-started/contributing/#acknowledging-contributions","text":"We follow the all-contributors specification and recognise various types of contributions. Take a look at our past and current contributors ! If you are a new contributor, please make sure we add you to our list of contributors. All contributions are recorded in .all-contributorsrc .","title":"Acknowledging contributions"},{"location":"getting-started/example/","text":"Example This example demonstrates using the raw OutlierDetection API to determine the outlierness of instances in the Thyroid Disease Dataset , which is part of the ODDS collection . We use OutlierDetectionData.jl to download and read the dataset. Import OutlierDetection and OutlierDetectionData using OutlierDetection : KNN , fit , transform , roc_auc using OutlierDetectionData : ODDS Download and read the \"thyroid\" dataset from the ODDS collection. X , y = ODDS . read ( \"thyroid\" ); # simply use the first 70% of the data for training n_train = convert ( Int , floor ( length ( y ) * 0.7 )); # split into train and test X_train , X_test , y_train , y_test = X [ : , begin : n_train - 1 ], X [ : , n_train : end ], y [ begin : n_train - 1 ], y [ n_train : end ]; Initialize an unsupervised KNN detector with k=10 neighbors. detector = KNN ( k = 10 ); Learn a model from the data X . model , scores_train = fit ( detector , X_train ); Evaluate our resulting scores on the training data. roc_auc ( y_train , scores_train ) julia> 0.945... Calculate the outlier scores for our test data. scores_test = transform ( detector , model , X_test ); Evaluate the result on the test data. roc_auc ( y_test , scores_test ) julia> 0.967... Notice Typically, you do not use the raw OutlierDetection API directly, but instead use MLJ to interface with OutlierDetection.jl . This way you can easily integrate and extend Julia's machine learning ecosystem with outlier detection algorithms. To learn more about the usage of MLJ, check out the documentation !","title":"Example"},{"location":"getting-started/example/#example","text":"This example demonstrates using the raw OutlierDetection API to determine the outlierness of instances in the Thyroid Disease Dataset , which is part of the ODDS collection . We use OutlierDetectionData.jl to download and read the dataset. Import OutlierDetection and OutlierDetectionData using OutlierDetection : KNN , fit , transform , roc_auc using OutlierDetectionData : ODDS Download and read the \"thyroid\" dataset from the ODDS collection. X , y = ODDS . read ( \"thyroid\" ); # simply use the first 70% of the data for training n_train = convert ( Int , floor ( length ( y ) * 0.7 )); # split into train and test X_train , X_test , y_train , y_test = X [ : , begin : n_train - 1 ], X [ : , n_train : end ], y [ begin : n_train - 1 ], y [ n_train : end ]; Initialize an unsupervised KNN detector with k=10 neighbors. detector = KNN ( k = 10 ); Learn a model from the data X . model , scores_train = fit ( detector , X_train ); Evaluate our resulting scores on the training data. roc_auc ( y_train , scores_train ) julia> 0.945... Calculate the outlier scores for our test data. scores_test = transform ( detector , model , X_test ); Evaluate the result on the test data. roc_auc ( y_test , scores_test ) julia> 0.967...","title":"Example"},{"location":"getting-started/example/#notice","text":"Typically, you do not use the raw OutlierDetection API directly, but instead use MLJ to interface with OutlierDetection.jl . This way you can easily integrate and extend Julia's machine learning ecosystem with outlier detection algorithms. To learn more about the usage of MLJ, check out the documentation !","title":"Notice"},{"location":"getting-started/installation/","text":"Installation It is recommended to use Pkg.jl for installation. Please make sure that you are using a compatible version of Julia. A list of compatible versions can be found our CI pipeline . Follow the command below to install the latest official release or use ] add OutlierDetection in the Julia REPL. import Pkg ; Pkg . add ( \"OutlierDetection\" ) A specific version can be installed by appending a version after a @ symbol, e.g. OutlierDetection@v0.1 . Additionally, you can directly install specific branches or commits by appending a # symbol and the corresponding branch name or commit SHA, e.g. OutlierDetection#master . If you would like to modify the package locally, you can use Pkg.develop(OutlierDetection) or ] dev OutlierDetection in the Julia REPL. This fetches a full clone of the package to ~/.julia/dev/ (the path can be changed by setting the environment variable JULIA_PKG_DEVDIR ).","title":"Installation"},{"location":"getting-started/installation/#installation","text":"It is recommended to use Pkg.jl for installation. Please make sure that you are using a compatible version of Julia. A list of compatible versions can be found our CI pipeline . Follow the command below to install the latest official release or use ] add OutlierDetection in the Julia REPL. import Pkg ; Pkg . add ( \"OutlierDetection\" ) A specific version can be installed by appending a version after a @ symbol, e.g. OutlierDetection@v0.1 . Additionally, you can directly install specific branches or commits by appending a # symbol and the corresponding branch name or commit SHA, e.g. OutlierDetection#master . If you would like to modify the package locally, you can use Pkg.develop(OutlierDetection) or ] dev OutlierDetection in the Julia REPL. This fetches a full clone of the package to ~/.julia/dev/ (the path can be changed by setting the environment variable JULIA_PKG_DEVDIR ).","title":"Installation"}]}