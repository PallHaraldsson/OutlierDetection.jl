{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"API/base/","text":"Base Here we define the abstract supertypes that all outlier detectors share as well as the necessary fit and score methods that have to be implemented for each detector. Types Detector # OutlierDetection.Detector \u2014 Type . Detector :: Union { <: SupervisedDetector , <: UnsupervisedDetector } The union type of all implemented detectors, including supervised, semi-supervised and unsupervised detectors. Note: A semi-supervised detector can be seen as a supervised detector with a specific class representing unlabeled data. source SupervisedDetector # OutlierDetection.SupervisedDetector \u2014 Type . SupervisedDetector This abstract type forms the basis for all implemented supervised outlier detection algorithms. To implement a new SupervisedDetector yourself, you have to implement the fit(detector, X, y)::Fit and score(detector, model, X)::Result methods. source UnsupervisedDetector # OutlierDetection.UnsupervisedDetector \u2014 Type . UnsupervisedDetector This abstract type forms the basis for all implemented unsupervised outlier detection algorithms. To implement a new UnsupervisedDetector yourself, you have to implement the fit(detector, X)::Fit and score(detector, model, X)::Result methods. source Classifier Missing docstring. Missing docstring for Classifier . Check Documenter's build log for details. Model # OutlierDetection.Model \u2014 Type . Model A Model represents the learned behaviour for specific Detector . This might include parameters in parametric models or other repesentations of the learned data in nonparametric models. In essence, it includes everything required to transform an instance to an outlier score. source Scores # OutlierDetection.Scores \u2014 Type . Scores :: AbstractVector { <: Real } Scores are continuous values, where the range depends on the specific detector yielding the scores. Note: All detectors return increasing scores and higher scores are associated with higher outlierness. Concretely, scores are defined as an AbstractVector{<:Real} . source Data # OutlierDetection.Data \u2014 Type . Data :: AbstractArray { <: Real } The raw input data for every detector is defined as AbstractArray{<:Real} and should be a one observation per column n-dimensional array. The input data used to fit a Detector and score Data . source Labels # OutlierDetection.Labels \u2014 Type . Labels :: AbstractVector { <: Integer } Labels are used for supervision and evaluation and are defined as an AbstractArray{<:Integer} . The convention for labels is that -1 indicates outliers, 1 indicates inliers and 0 indicates unlabeled data in semi-supervised tasks. source Fit # OutlierDetection.Fit \u2014 Type . Fit A Fit bundles a Model and Scores achieved when fitting a Detector . The model is used directly in later score calls and the (train-) scores are forwarded in score . source Result # OutlierDetection.Result \u2014 Type . Result :: Tuple { Scores , Scores } Describes the result of using score with a Detector and is a tuple containing the train scores achieved with fit and the test scores achieved with score . We return both train and test scores because this gives us the greatest flexibility in later score combination or classification. source Functions fit # OutlierDetection.fit \u2014 Function . fit ( detector , X , y ) Fit a specified unsupervised, supervised or semi-supervised outlier detector. That is, learn a Model from input data X and, in the supervised and semi-supervised setting, labels y . In a supervised setting, the label -1 represents outliers and 1 inliers. In a semi-supervised setting, the label 0 additionally represents unlabeled data. Note: Unsupervised detectors can be fitted without specifying y , otherwise y is simply ignore. Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. X::Union{AbstractMatrix, Tables.jl-compatible} Either a matrix or a Tables.jl-compatible data source, with one observation per row and a number of feature columns. Returns fit::Fit The learned model of the given detector, which contains all the necessary information for later prediction and the achieved outlier scores of the given input data X . Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) source score # OutlierDetection.score \u2014 Function . score ( detector , model , X ) Transform input data X to outlier scores using an UnsupervisedDetector or SupervisedDetector and a corresponding Model . Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. model::Model The model learned from using fit with a supervised or unsupervised Detector X::Union{AbstractMatrix, Tables.jl-compatible} Either a matrix or a Tables.jl-compatible data source, with one observation per row and a number of feature columns. Returns result::Result Tuple of the achieved outlier scores of the given train and test data. Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) source detect # OutlierDetection.detect \u2014 Function . detect ( Evaluator , result ... ) Convert a number of scores into inlier ( 1 ) / outlier ( -1 ) classes, typically by using on a outlier-threshold on the achieved scores. Parameters evaluator::Evaluator A Evaluator that implements the detect method. result::Result... One or more score results (tuples) or alternatively a single vector of scores or two vectors, where the first vector represents train scores and the second vector test scores. Returns result::Labels A vector containing the binary inlier and outlier labels. Examples using OutlierDetection : Class , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Class (), train_scores , test_scores ) # or, if using multiple detectors # y\u0302 = detect(Class(), (train1, test1), (train2, test2), ...) source","title":"Base"},{"location":"API/base/#base","text":"Here we define the abstract supertypes that all outlier detectors share as well as the necessary fit and score methods that have to be implemented for each detector.","title":"Base"},{"location":"API/base/#types","text":"","title":"Types"},{"location":"API/base/#detector","text":"# OutlierDetection.Detector \u2014 Type . Detector :: Union { <: SupervisedDetector , <: UnsupervisedDetector } The union type of all implemented detectors, including supervised, semi-supervised and unsupervised detectors. Note: A semi-supervised detector can be seen as a supervised detector with a specific class representing unlabeled data. source","title":"Detector"},{"location":"API/base/#superviseddetector","text":"# OutlierDetection.SupervisedDetector \u2014 Type . SupervisedDetector This abstract type forms the basis for all implemented supervised outlier detection algorithms. To implement a new SupervisedDetector yourself, you have to implement the fit(detector, X, y)::Fit and score(detector, model, X)::Result methods. source","title":"SupervisedDetector"},{"location":"API/base/#unsuperviseddetector","text":"# OutlierDetection.UnsupervisedDetector \u2014 Type . UnsupervisedDetector This abstract type forms the basis for all implemented unsupervised outlier detection algorithms. To implement a new UnsupervisedDetector yourself, you have to implement the fit(detector, X)::Fit and score(detector, model, X)::Result methods. source","title":"UnsupervisedDetector"},{"location":"API/base/#classifier","text":"Missing docstring. Missing docstring for Classifier . Check Documenter's build log for details.","title":"Classifier"},{"location":"API/base/#model","text":"# OutlierDetection.Model \u2014 Type . Model A Model represents the learned behaviour for specific Detector . This might include parameters in parametric models or other repesentations of the learned data in nonparametric models. In essence, it includes everything required to transform an instance to an outlier score. source","title":"Model"},{"location":"API/base/#scores","text":"# OutlierDetection.Scores \u2014 Type . Scores :: AbstractVector { <: Real } Scores are continuous values, where the range depends on the specific detector yielding the scores. Note: All detectors return increasing scores and higher scores are associated with higher outlierness. Concretely, scores are defined as an AbstractVector{<:Real} . source","title":"Scores"},{"location":"API/base/#data","text":"# OutlierDetection.Data \u2014 Type . Data :: AbstractArray { <: Real } The raw input data for every detector is defined as AbstractArray{<:Real} and should be a one observation per column n-dimensional array. The input data used to fit a Detector and score Data . source","title":"Data"},{"location":"API/base/#labels","text":"# OutlierDetection.Labels \u2014 Type . Labels :: AbstractVector { <: Integer } Labels are used for supervision and evaluation and are defined as an AbstractArray{<:Integer} . The convention for labels is that -1 indicates outliers, 1 indicates inliers and 0 indicates unlabeled data in semi-supervised tasks. source","title":"Labels"},{"location":"API/base/#fit","text":"# OutlierDetection.Fit \u2014 Type . Fit A Fit bundles a Model and Scores achieved when fitting a Detector . The model is used directly in later score calls and the (train-) scores are forwarded in score . source","title":"Fit"},{"location":"API/base/#result","text":"# OutlierDetection.Result \u2014 Type . Result :: Tuple { Scores , Scores } Describes the result of using score with a Detector and is a tuple containing the train scores achieved with fit and the test scores achieved with score . We return both train and test scores because this gives us the greatest flexibility in later score combination or classification. source","title":"Result"},{"location":"API/base/#functions","text":"","title":"Functions"},{"location":"API/base/#fit_1","text":"# OutlierDetection.fit \u2014 Function . fit ( detector , X , y ) Fit a specified unsupervised, supervised or semi-supervised outlier detector. That is, learn a Model from input data X and, in the supervised and semi-supervised setting, labels y . In a supervised setting, the label -1 represents outliers and 1 inliers. In a semi-supervised setting, the label 0 additionally represents unlabeled data. Note: Unsupervised detectors can be fitted without specifying y , otherwise y is simply ignore. Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. X::Union{AbstractMatrix, Tables.jl-compatible} Either a matrix or a Tables.jl-compatible data source, with one observation per row and a number of feature columns. Returns fit::Fit The learned model of the given detector, which contains all the necessary information for later prediction and the achieved outlier scores of the given input data X . Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) source","title":"fit"},{"location":"API/base/#score","text":"# OutlierDetection.score \u2014 Function . score ( detector , model , X ) Transform input data X to outlier scores using an UnsupervisedDetector or SupervisedDetector and a corresponding Model . Parameters detector::Detector Any UnsupervisedDetector or SupervisedDetector implementation. model::Model The model learned from using fit with a supervised or unsupervised Detector X::Union{AbstractMatrix, Tables.jl-compatible} Either a matrix or a Tables.jl-compatible data source, with one observation per row and a number of feature columns. Returns result::Result Tuple of the achieved outlier scores of the given train and test data. Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) source","title":"score"},{"location":"API/base/#detect","text":"# OutlierDetection.detect \u2014 Function . detect ( Evaluator , result ... ) Convert a number of scores into inlier ( 1 ) / outlier ( -1 ) classes, typically by using on a outlier-threshold on the achieved scores. Parameters evaluator::Evaluator A Evaluator that implements the detect method. result::Result... One or more score results (tuples) or alternatively a single vector of scores or two vectors, where the first vector represents train scores and the second vector test scores. Returns result::Labels A vector containing the binary inlier and outlier labels. Examples using OutlierDetection : Class , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Class (), train_scores , test_scores ) # or, if using multiple detectors # y\u0302 = detect(Class(), (train1, test1), (train2, test2), ...) source","title":"detect"},{"location":"API/classifiers/","text":"Classifiers Detectors assign a real-valued outlier score to each instance. A classifier simply takes the training and test scores of one or more detectors and converts them to binary labels (classes). The classes in outlier detection traditionally consist of 1 for inliers, -1 for outliers. Class # OutlierDetection.Class \u2014 Type . Class ( outlier_fraction = 0.1 , classify = classify , combine = combine , normalize = normalize ) A flexible, quantile-thresholding classifier that maps the outlier scores of a single or multiple outlier detection models to binary classes, where 1 represents inliers and -1 represents outliers. Parameters outlier_fraction::Float64 The fraction of outliers (number between 0 and 1) in the data is used to determine the score threshold to classify the samples into inliers and outliers. classify::Union{Function, Nothing} A function to transform a vector of scores to a vector of bits, where 1 represents an outlier and 0 represents a normal instance. Hint: Sometimes you don't want to transform your scores to classes, e.g. in ROC AUC evaluation, where you can use no_classify to pass along the reduced (raw) scores. See classify for a specific implementation. combine::Function A function to reduce a matrix, where each row represents an instance and each column represents a score of specific detector, to a vector of scores for each instance. See combine for a specific implementation. Note: This function is not called if the input to the evaluator consists of a single train/test scores tuple. normalize::Union{Function, Nothing} A function to reduce a matrix, where each row represents an instance and each column a score of specific detector, to a vector of scores for each instance. See normalize for a specific implementation. Examples using OutlierDetection : Class , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Class (), train_scores , test_scores ) # or, if using multiple detectors # y\u0302 = detect(Class(), (train1, test1), (train2, test2), ...) source","title":"Classifiers"},{"location":"API/classifiers/#classifiers","text":"Detectors assign a real-valued outlier score to each instance. A classifier simply takes the training and test scores of one or more detectors and converts them to binary labels (classes). The classes in outlier detection traditionally consist of 1 for inliers, -1 for outliers.","title":"Classifiers"},{"location":"API/classifiers/#class","text":"# OutlierDetection.Class \u2014 Type . Class ( outlier_fraction = 0.1 , classify = classify , combine = combine , normalize = normalize ) A flexible, quantile-thresholding classifier that maps the outlier scores of a single or multiple outlier detection models to binary classes, where 1 represents inliers and -1 represents outliers. Parameters outlier_fraction::Float64 The fraction of outliers (number between 0 and 1) in the data is used to determine the score threshold to classify the samples into inliers and outliers. classify::Union{Function, Nothing} A function to transform a vector of scores to a vector of bits, where 1 represents an outlier and 0 represents a normal instance. Hint: Sometimes you don't want to transform your scores to classes, e.g. in ROC AUC evaluation, where you can use no_classify to pass along the reduced (raw) scores. See classify for a specific implementation. combine::Function A function to reduce a matrix, where each row represents an instance and each column represents a score of specific detector, to a vector of scores for each instance. See combine for a specific implementation. Note: This function is not called if the input to the evaluator consists of a single train/test scores tuple. normalize::Union{Function, Nothing} A function to reduce a matrix, where each row represents an instance and each column a score of specific detector, to a vector of scores for each instance. See normalize for a specific implementation. Examples using OutlierDetection : Class , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Class (), train_scores , test_scores ) # or, if using multiple detectors # y\u0302 = detect(Class(), (train1, test1), (train2, test2), ...) source","title":"Class"},{"location":"API/detectors/","text":"Detectors A Detector is just a mutable collection of hyperparameters. Each detector implements a fit and score method, where fit refers to learning a model from training data and score refers to using a learned model to calculate outlier scores of new data. Detectors typically do not classify samples; that's why a classifier is used to convert the scores into binary labels, see Class for example. ABOD # OutlierDetection.ABOD \u2014 Type . ABOD ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false , enhanced = false ) Determine outliers based on the angles to its nearest neighbors. This implements the FastABOD variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. Notice: The scores are inverted, to conform to our notion that higher scores describe higher outlierness. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. enhanced::Bool When enhanced=true , it uses the enhanced ABOD (EABOD) adaptation proposed by [2]. Examples using OutlierDetection : ABOD , fit , score detector = ABOD () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data. [2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships. source AE # OutlierDetection.AE \u2014 Type . AE ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), loss = mse ) Calculate the anomaly score of an instance based on the reconstruction loss of an autoencoder, see [1] for an explanation of auto encoders. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. Examples using OutlierDetection : AE , fit , score detector = AE () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Aggarwal, Charu C. (2017): Outlier Analysis. source COF # OutlierDetection.COF \u2014 Type . COF ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Local outlier density based on chaining distance between graphs of neighbors, as described in [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : COF , fit , score detector = COF () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns. source DeepSAD # OutlierDetection.DeepSAD \u2014 Type . DeepSAD ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = true , partial = false , opt = ADAM (), loss = mse , eta = 1 , eps = 1e-6 , callback = _ -> () -> ()) Deep Semi-Supervised Anomaly detection technique based on the distance to a hypersphere center as described in [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. eta::Real Weighting parameter for the labeled data; i.e. higher values of eta assign higher weight to labeled data in the svdd loss function. For a sensitivity analysis of this parameter, see [1]. eps::Real Because the inverse distance used in the svdd loss can lead to division by zero, the parameters eps is added for numerical stability. callback::Function Experimental parameter that might change . A function to be called after the model parameters have been updated that can call Flux's callback helpers, see https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1 . Notice: The parameters batchsize , epochs , shuffle , partial , opt and callback can also be tuples of size 2, specifying the corresponding values for (1) pretraining and (2) training; otherwise the same values are used for pretraining and training. Examples using OutlierDetection : DeepSAD , fit , score detector = DeepSAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model = fit ( detector , X , y ) train_scores , test_scores = score ( detector , model , X ) References [1] Ruff, Lukas; Vandermeulen, Robert A.; G\u00f6rnitz, Nico; Binder, Alexander; M\u00fcller, Emmanuel; M\u00fcller, Klaus-Robert; Kloft, Marius (2019): Deep Semi-Supervised Anomaly Detection. source DNN # OutlierDetection.DNN \u2014 Type . DNN ( d = 0 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Anomaly score based on the number of neighbors in a hypersphere of radius d . Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper. Parameters d::Real The hypersphere radius used to calculate the global density of an instance. metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : DNN , fit , score detector = DNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets. source ESAD # OutlierDetection.ESAD \u2014 Type . ESAD ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), \u03bb1 = 1 , \u03bb2 = 1 , noise = identity ) End-to-End semi-supervised anomaly detection algorithm similar to DeepSAD, but without the pretraining phase. The algorithm was published by Huang et al., see [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. \u03bb1::Real Weighting parameter of the norm loss, which minimizes the empirical variance and thus minimizes entropy. \u03bb2::Real Weighting parameter of the assistent loss function to define the consistency between the two encoders. noise::Function (AbstractArray{T} -> AbstractArray{T}) A function to be applied to a batch of input data to add noise, see [1] for an explanation. Examples using OutlierDetection : ESAD , fit , score detector = ESAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model = fit ( detector , X , y ) train_scores , test_scores = score ( detector , model , X ) References [1] Huang, Chaoqin; Ye, Fei; Zhang, Ya; Wang, Yan-Feng; Tian, Qi (2020): ESAD: End-to-end Deep Semi-supervised Anomaly Detection. source KNN # OutlierDetection.KNN \u2014 Type . KNN ( k = 5 , metric = Euclidean , algorithm =: kdtree , leafsize = 10 , reorder = true , reduction =: maximum ) Calculate the anomaly score of an instance based on the distance to its k-nearest neighbors. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. reduction::Symbol One of (:maximum, :median, :mean) . ( reduction=:maximum ) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability. Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets. [2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces. source LOF # OutlierDetection.LOF \u2014 Type . LOF ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Calculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : LOF , fit , score detector = LOF () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, J\u00f6rg (2000): LOF: Identifying Density-Based Local Outliers. source","title":"Detectors"},{"location":"API/detectors/#detectors","text":"A Detector is just a mutable collection of hyperparameters. Each detector implements a fit and score method, where fit refers to learning a model from training data and score refers to using a learned model to calculate outlier scores of new data. Detectors typically do not classify samples; that's why a classifier is used to convert the scores into binary labels, see Class for example.","title":"Detectors"},{"location":"API/detectors/#abod","text":"# OutlierDetection.ABOD \u2014 Type . ABOD ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false , enhanced = false ) Determine outliers based on the angles to its nearest neighbors. This implements the FastABOD variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. Notice: The scores are inverted, to conform to our notion that higher scores describe higher outlierness. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. enhanced::Bool When enhanced=true , it uses the enhanced ABOD (EABOD) adaptation proposed by [2]. Examples using OutlierDetection : ABOD , fit , score detector = ABOD () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data. [2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships. source","title":"ABOD"},{"location":"API/detectors/#ae","text":"# OutlierDetection.AE \u2014 Type . AE ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), loss = mse ) Calculate the anomaly score of an instance based on the reconstruction loss of an autoencoder, see [1] for an explanation of auto encoders. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. Examples using OutlierDetection : AE , fit , score detector = AE () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Aggarwal, Charu C. (2017): Outlier Analysis. source","title":"AE"},{"location":"API/detectors/#cof","text":"# OutlierDetection.COF \u2014 Type . COF ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Local outlier density based on chaining distance between graphs of neighbors, as described in [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : COF , fit , score detector = COF () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns. source","title":"COF"},{"location":"API/detectors/#deepsad","text":"# OutlierDetection.DeepSAD \u2014 Type . DeepSAD ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = true , partial = false , opt = ADAM (), loss = mse , eta = 1 , eps = 1e-6 , callback = _ -> () -> ()) Deep Semi-Supervised Anomaly detection technique based on the distance to a hypersphere center as described in [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. loss::Function The loss function used to calculate the reconstruction error, see https://fluxml.ai/Flux.jl/stable/models/losses/ for examples. eta::Real Weighting parameter for the labeled data; i.e. higher values of eta assign higher weight to labeled data in the svdd loss function. For a sensitivity analysis of this parameter, see [1]. eps::Real Because the inverse distance used in the svdd loss can lead to division by zero, the parameters eps is added for numerical stability. callback::Function Experimental parameter that might change . A function to be called after the model parameters have been updated that can call Flux's callback helpers, see https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1 . Notice: The parameters batchsize , epochs , shuffle , partial , opt and callback can also be tuples of size 2, specifying the corresponding values for (1) pretraining and (2) training; otherwise the same values are used for pretraining and training. Examples using OutlierDetection : DeepSAD , fit , score detector = DeepSAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model = fit ( detector , X , y ) train_scores , test_scores = score ( detector , model , X ) References [1] Ruff, Lukas; Vandermeulen, Robert A.; G\u00f6rnitz, Nico; Binder, Alexander; M\u00fcller, Emmanuel; M\u00fcller, Klaus-Robert; Kloft, Marius (2019): Deep Semi-Supervised Anomaly Detection. source","title":"DeepSAD"},{"location":"API/detectors/#dnn","text":"# OutlierDetection.DNN \u2014 Type . DNN ( d = 0 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Anomaly score based on the number of neighbors in a hypersphere of radius d . Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper. Parameters d::Real The hypersphere radius used to calculate the global density of an instance. metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : DNN , fit , score detector = DNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets. source","title":"DNN"},{"location":"API/detectors/#esad","text":"# OutlierDetection.ESAD \u2014 Type . ESAD ( encoder = Chain (), decoder = Chain (), batchsize = 32 , epochs = 1 , shuffle = false , partial = true , opt = ADAM (), \u03bb1 = 1 , \u03bb2 = 1 , noise = identity ) End-to-End semi-supervised anomaly detection algorithm similar to DeepSAD, but without the pretraining phase. The algorithm was published by Huang et al., see [1]. Parameters encoder::Chain Transforms the input data into a latent state with a fixed shape. decoder::Chain Transforms the latent state back into the shape of the input data. batchsize::Integer The number of samples to work through before updating the internal model parameters. epochs::Integer The number of passes of the entire training dataset the machine learning algorithm has completed. shuffle::Bool If shuffle=true , shuffles the observations each time iterations are re-started, else no shuffling is performed. partial::Bool If partial=false , drops the last mini-batch if it is smaller than the batchsize. opt::Any Any Flux-compatibale optimizer, typically a struct that holds all the optimiser parameters along with a definition of apply! that defines how to apply the update rule associated with the optimizer. \u03bb1::Real Weighting parameter of the norm loss, which minimizes the empirical variance and thus minimizes entropy. \u03bb2::Real Weighting parameter of the assistent loss function to define the consistency between the two encoders. noise::Function (AbstractArray{T} -> AbstractArray{T}) A function to be applied to a batch of input data to add noise, see [1] for an explanation. Examples using OutlierDetection : ESAD , fit , score detector = ESAD () X = rand ( 10 , 100 ) y = rand ([ - 1 , 1 ], 100 ) model = fit ( detector , X , y ) train_scores , test_scores = score ( detector , model , X ) References [1] Huang, Chaoqin; Ye, Fei; Zhang, Ya; Wang, Yan-Feng; Tian, Qi (2020): ESAD: End-to-end Deep Semi-supervised Anomaly Detection. source","title":"ESAD"},{"location":"API/detectors/#knn","text":"# OutlierDetection.KNN \u2014 Type . KNN ( k = 5 , metric = Euclidean , algorithm =: kdtree , leafsize = 10 , reorder = true , reduction =: maximum ) Calculate the anomaly score of an instance based on the distance to its k-nearest neighbors. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. reduction::Symbol One of (:maximum, :median, :mean) . ( reduction=:maximum ) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability. Examples using OutlierDetection : KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets. [2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces. source","title":"KNN"},{"location":"API/detectors/#lof","text":"# OutlierDetection.LOF \u2014 Type . LOF ( k = 5 , metric = Euclidean (), algorithm = : kdtree , leafsize = 10 , reorder = true , parallel = false ) Calculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1]. Parameters k::Integer Number of neighbors (must be greater than 0). metric::Metric This is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric. algorithm::Symbol One of (:kdtree, :balltree) . In a kdtree , points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric. leafsize::Int Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points. reorder::Bool While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true. parallel::Bool Parallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel. Examples using OutlierDetection : LOF , fit , score detector = LOF () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) References [1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, J\u00f6rg (2000): LOF: Identifying Density-Based Local Outliers. source","title":"LOF"},{"location":"API/evaluation/","text":"Evaluation Raw evaluation methods that are typically used by an Evaluator , see Class for an exemplary usage. Evaluators Class Missing docstring. Missing docstring for Class . Check Documenter's build log for details. Score # OutlierDetection.Score \u2014 Type . Score ( combine = combine , normalize = normalize ) Transform the results of a single or multiple outlier detection models to combined and normalized scores. Parameters combine::Function A function to reduce a matrix, where each row represents an instance and each column represents a score of specific detector, to a vector of scores for each instance. See combine for a specific implementation. Note: This function is not called if the input to the evaluator consists of a single train/test scores tuple. normalize::Union{Function, Nothing} A function to reduce a matrix, where each row represents an instance and each column a score of specific detector, to a vector of scores for each instance. See normalize for a specific implementation. Examples using OutlierDetection : Score , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Score (), train_scores , test_scores ) source Score Combination combine # OutlierDetection.combine \u2014 Function . combine ( scores_mat , strategy = : mean ) Combination method to merge outlier scores from multiple detectors by using a combination strateg. This function is typically used within a Evaluator . TODO: Add AOM/MOA/LSCP combination strategies Parameters scores_mat::AbstractMatrix{T} A matrix, with each row representing the scores for a specific instance and each column representing a detector. strategy::Symbol=:mean Determines how to combine the scores of multiple detectors, e.g. maximum or mean of all scores. Returns combined_scores::AbstractVector{T} The combined scores.e.g. the maximum of scores from different detectors. Examples scores = [1 2; 3 4; 5 6] combine(scores) # [1.5, 3.5, 5.5] source Score Normalization normalize # OutlierDetection.normalize \u2014 Function . normalize ( scoresTrain , scoresTest ) Transform an array of scores into a range between [0,1] using min-max scaling. Parameters scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns normalized_scores::Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}} The normalized train and test scores. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) normalize(scores_train, scores_test) # ([0.0, 0.5, 1.0], [1.0, 1.0, 0.5, 0.0, 0.0]) normalize(scores_train) # [0.0, 0.5, 1.0] source unify # OutlierDetection.unify \u2014 Function . unify ( scores_train , scores_test ) Transform an array of scores into a range between [0,1] using unifying scores as described in [1]. Parameters scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns unified_scores::Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}} The unified train and test scores. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) unify(scores_train, scores_test) # ([0.0, 0.0, 0.68..], [0.95.., 0.68.., 0.0, 0.0, 0.0]) unify(scores_train) # [0.0, 0.0, 0.68..] References Kriegel, Hans-Peter; Kroger, Peer; Schubert, Erich; Zimek, Arthur (2011): Interpreting and Unifying Outlier Scores. source Score Thresholding classify # OutlierDetection.classify \u2014 Function . classify ( outlier_fraction , scores_train , scores_test ) Convert an array of scores to an array of classes with 1 indicating normal data and -1 indicating outliers. The conversion is based on percentiles of the training data, i.e. all datapoints above the 1 - outlier_fraction percentile are considered outliers. Parameters outlier_fraction::Real The fraction of outliers (number between 0 and 1) in the data used to determine the score threshold to classify the samples into inliers and outliers. scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns classes::AbstractVector{<:Integer} The vector of classes consisting of -1 (outlier) and 1 (inlier) elements. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) classify(0.3, scores_train, scores_test) # [-1, -1, 1, 1, 1] classify(0.3, scores_train) # [1, 1, -1] source","title":"Evaluation"},{"location":"API/evaluation/#evaluation","text":"Raw evaluation methods that are typically used by an Evaluator , see Class for an exemplary usage.","title":"Evaluation"},{"location":"API/evaluation/#evaluators","text":"","title":"Evaluators"},{"location":"API/evaluation/#class","text":"Missing docstring. Missing docstring for Class . Check Documenter's build log for details.","title":"Class"},{"location":"API/evaluation/#score","text":"# OutlierDetection.Score \u2014 Type . Score ( combine = combine , normalize = normalize ) Transform the results of a single or multiple outlier detection models to combined and normalized scores. Parameters combine::Function A function to reduce a matrix, where each row represents an instance and each column represents a score of specific detector, to a vector of scores for each instance. See combine for a specific implementation. Note: This function is not called if the input to the evaluator consists of a single train/test scores tuple. normalize::Union{Function, Nothing} A function to reduce a matrix, where each row represents an instance and each column a score of specific detector, to a vector of scores for each instance. See normalize for a specific implementation. Examples using OutlierDetection : Score , KNN , fit , score detector = KNN () X = rand ( 10 , 100 ) model = fit ( detector , X ) train_scores , test_scores = score ( detector , model , X ) y\u0302 = detect ( Score (), train_scores , test_scores ) source","title":"Score"},{"location":"API/evaluation/#score-combination","text":"","title":"Score Combination"},{"location":"API/evaluation/#combine","text":"# OutlierDetection.combine \u2014 Function . combine ( scores_mat , strategy = : mean ) Combination method to merge outlier scores from multiple detectors by using a combination strateg. This function is typically used within a Evaluator . TODO: Add AOM/MOA/LSCP combination strategies Parameters scores_mat::AbstractMatrix{T} A matrix, with each row representing the scores for a specific instance and each column representing a detector. strategy::Symbol=:mean Determines how to combine the scores of multiple detectors, e.g. maximum or mean of all scores. Returns combined_scores::AbstractVector{T} The combined scores.e.g. the maximum of scores from different detectors. Examples scores = [1 2; 3 4; 5 6] combine(scores) # [1.5, 3.5, 5.5] source","title":"combine"},{"location":"API/evaluation/#score-normalization","text":"","title":"Score Normalization"},{"location":"API/evaluation/#normalize","text":"# OutlierDetection.normalize \u2014 Function . normalize ( scoresTrain , scoresTest ) Transform an array of scores into a range between [0,1] using min-max scaling. Parameters scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns normalized_scores::Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}} The normalized train and test scores. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) normalize(scores_train, scores_test) # ([0.0, 0.5, 1.0], [1.0, 1.0, 0.5, 0.0, 0.0]) normalize(scores_train) # [0.0, 0.5, 1.0] source","title":"normalize"},{"location":"API/evaluation/#unify","text":"# OutlierDetection.unify \u2014 Function . unify ( scores_train , scores_test ) Transform an array of scores into a range between [0,1] using unifying scores as described in [1]. Parameters scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns unified_scores::Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}} The unified train and test scores. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) unify(scores_train, scores_test) # ([0.0, 0.0, 0.68..], [0.95.., 0.68.., 0.0, 0.0, 0.0]) unify(scores_train) # [0.0, 0.0, 0.68..] References Kriegel, Hans-Peter; Kroger, Peer; Schubert, Erich; Zimek, Arthur (2011): Interpreting and Unifying Outlier Scores. source","title":"unify"},{"location":"API/evaluation/#score-thresholding","text":"","title":"Score Thresholding"},{"location":"API/evaluation/#classify","text":"# OutlierDetection.classify \u2014 Function . classify ( outlier_fraction , scores_train , scores_test ) Convert an array of scores to an array of classes with 1 indicating normal data and -1 indicating outliers. The conversion is based on percentiles of the training data, i.e. all datapoints above the 1 - outlier_fraction percentile are considered outliers. Parameters outlier_fraction::Real The fraction of outliers (number between 0 and 1) in the data used to determine the score threshold to classify the samples into inliers and outliers. scores_train::AbstractVector{<:Real} A vector of training scores, typically the result of fit with a detector. scores_test::AbstractVector{<:Real} A vector of test scores, typically the result of score using a previously fitted detector. Returns classes::AbstractVector{<:Integer} The vector of classes consisting of -1 (outlier) and 1 (inlier) elements. Examples scores_train, scores_test = ([1, 2, 3], [4, 3, 2, 1, 0]) classify(0.3, scores_train, scores_test) # [-1, -1, 1, 1, 1] classify(0.3, scores_train) # [1, 1, -1] source","title":"classify"},{"location":"API/pydetectors/","text":"Python Detectors Using PyCall , we can easily integrate existing python outlier detection algorithms. Currently, almost every PyOD algorithm is integrated and can thus be easily used directly from Julia. As a naming convention, every python detector is named Py$NAME . PyABOD # OutlierDetection.PyABOD \u2014 Type . PyABOD ( n_neighbors = 5 , method = \"fast\" ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod source PyCBLOF # OutlierDetection.PyCBLOF \u2014 Type . PyCBLOF ( n_clusters = 8 , alpha = 0.9 , beta = 5 , use_weights = false , random_state = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof source PyCOF # OutlierDetection.PyCOF \u2014 Type . PyCOF ( n_neighbors = 5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof source PyCOPOD # OutlierDetection.PyCOPOD \u2014 Type . PyCOPOD () https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod source PyHBOS # OutlierDetection.PyHBOS \u2014 Type . PyHBOS ( n_bins = 10 , alpha = 0.1 , tol = 0.5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos source PyIForest # OutlierDetection.PyIForest \u2014 Type . PyIForest ( n_estimators = 100 , max_samples = \"auto\" , max_features = 1.0 bootstrap = false , behaviour = \"new\" , random_state = nothing , verbose = 0 , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest source PyKNN # OutlierDetection.PyKNN \u2014 Type . PyKNN ( n_neighbors = 5 , method = \"largest\" , radius = 1.0 , algorithm = \"auto\" , leaf_size = 30 , metric = \"minkowski\" , p = 2 , metric_params = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn source PyLMDD # OutlierDetection.PyLMDD \u2014 Type . PyLMDD ( n_iter = 50 , dis_measure = \"aad\" , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd source PyLODA # OutlierDetection.PyLODA \u2014 Type . PyLODA ( n_bins = 10 , n_random_cuts = 100 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda source PyLOF # OutlierDetection.PyLOF \u2014 Type . PyLOF ( n_neighbors = 5 , method = \"largest\" , algorithm = \"auto\" , leaf_size = 30 , metric = \"minkowski\" , p = 2 , metric_params = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof source PyLOCI # OutlierDetection.PyLOCI \u2014 Type . PyLOCI ( alpha = 0.5 , k = 3 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci source PyMCD # OutlierDetection.PyMCD \u2014 Type . PyMCD ( store_precision = true , assume_centered = false , support_fraction = nothing , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd source PyOCSVM # OutlierDetection.PyOCSVM \u2014 Type . PyOCSVM ( kernel = \"rbf\" , degree = 3 , gamma = \"auto\" , coef0 = 0.0 , tol = 0.001 , nu = 0.5 , shrinking = true , cache_size = 200 , verbose = false , max_iter = - 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm source PyPCA # OutlierDetection.PyPCA \u2014 Type . PyPCA ( n_components = nothing , n_selected_components = nothing , copy = true , whiten = false , svd_solver = \"auto\" , tol = 0.0 iterated_power = \"auto\" , standardization = true , weighted = true , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca source PyROD # OutlierDetection.PyROD \u2014 Type . PyROD ( parallel_execution = false ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod source PySOD # OutlierDetection.PySOD \u2014 Type . PySOD ( n_neighbors = 5 , ref_set = 10 , alpha = 0.8 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod source PySOS # OutlierDetection.PySOS \u2014 Type . PySOS ( perplexity = 4.5 , metric = \"minkowski\" , eps = 1e-5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos source","title":"PyDetectors"},{"location":"API/pydetectors/#python-detectors","text":"Using PyCall , we can easily integrate existing python outlier detection algorithms. Currently, almost every PyOD algorithm is integrated and can thus be easily used directly from Julia. As a naming convention, every python detector is named Py$NAME .","title":"Python Detectors"},{"location":"API/pydetectors/#pyabod","text":"# OutlierDetection.PyABOD \u2014 Type . PyABOD ( n_neighbors = 5 , method = \"fast\" ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod source","title":"PyABOD"},{"location":"API/pydetectors/#pycblof","text":"# OutlierDetection.PyCBLOF \u2014 Type . PyCBLOF ( n_clusters = 8 , alpha = 0.9 , beta = 5 , use_weights = false , random_state = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof source","title":"PyCBLOF"},{"location":"API/pydetectors/#pycof","text":"# OutlierDetection.PyCOF \u2014 Type . PyCOF ( n_neighbors = 5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof source","title":"PyCOF"},{"location":"API/pydetectors/#pycopod","text":"# OutlierDetection.PyCOPOD \u2014 Type . PyCOPOD () https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod source","title":"PyCOPOD"},{"location":"API/pydetectors/#pyhbos","text":"# OutlierDetection.PyHBOS \u2014 Type . PyHBOS ( n_bins = 10 , alpha = 0.1 , tol = 0.5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos source","title":"PyHBOS"},{"location":"API/pydetectors/#pyiforest","text":"# OutlierDetection.PyIForest \u2014 Type . PyIForest ( n_estimators = 100 , max_samples = \"auto\" , max_features = 1.0 bootstrap = false , behaviour = \"new\" , random_state = nothing , verbose = 0 , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest source","title":"PyIForest"},{"location":"API/pydetectors/#pyknn","text":"# OutlierDetection.PyKNN \u2014 Type . PyKNN ( n_neighbors = 5 , method = \"largest\" , radius = 1.0 , algorithm = \"auto\" , leaf_size = 30 , metric = \"minkowski\" , p = 2 , metric_params = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn source","title":"PyKNN"},{"location":"API/pydetectors/#pylmdd","text":"# OutlierDetection.PyLMDD \u2014 Type . PyLMDD ( n_iter = 50 , dis_measure = \"aad\" , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd source","title":"PyLMDD"},{"location":"API/pydetectors/#pyloda","text":"# OutlierDetection.PyLODA \u2014 Type . PyLODA ( n_bins = 10 , n_random_cuts = 100 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda source","title":"PyLODA"},{"location":"API/pydetectors/#pylof","text":"# OutlierDetection.PyLOF \u2014 Type . PyLOF ( n_neighbors = 5 , method = \"largest\" , algorithm = \"auto\" , leaf_size = 30 , metric = \"minkowski\" , p = 2 , metric_params = nothing , n_jobs = 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof source","title":"PyLOF"},{"location":"API/pydetectors/#pyloci","text":"# OutlierDetection.PyLOCI \u2014 Type . PyLOCI ( alpha = 0.5 , k = 3 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci source","title":"PyLOCI"},{"location":"API/pydetectors/#pymcd","text":"# OutlierDetection.PyMCD \u2014 Type . PyMCD ( store_precision = true , assume_centered = false , support_fraction = nothing , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd source","title":"PyMCD"},{"location":"API/pydetectors/#pyocsvm","text":"# OutlierDetection.PyOCSVM \u2014 Type . PyOCSVM ( kernel = \"rbf\" , degree = 3 , gamma = \"auto\" , coef0 = 0.0 , tol = 0.001 , nu = 0.5 , shrinking = true , cache_size = 200 , verbose = false , max_iter = - 1 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm source","title":"PyOCSVM"},{"location":"API/pydetectors/#pypca","text":"# OutlierDetection.PyPCA \u2014 Type . PyPCA ( n_components = nothing , n_selected_components = nothing , copy = true , whiten = false , svd_solver = \"auto\" , tol = 0.0 iterated_power = \"auto\" , standardization = true , weighted = true , random_state = nothing ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca source","title":"PyPCA"},{"location":"API/pydetectors/#pyrod","text":"# OutlierDetection.PyROD \u2014 Type . PyROD ( parallel_execution = false ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod source","title":"PyROD"},{"location":"API/pydetectors/#pysod","text":"# OutlierDetection.PySOD \u2014 Type . PySOD ( n_neighbors = 5 , ref_set = 10 , alpha = 0.8 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod source","title":"PySOD"},{"location":"API/pydetectors/#pysos","text":"# OutlierDetection.PySOS \u2014 Type . PySOS ( perplexity = 4.5 , metric = \"minkowski\" , eps = 1e-5 ) https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos source","title":"PySOS"},{"location":"documentation/architecture/","text":"Architecture TODO","title":"Architecture"},{"location":"documentation/architecture/#architecture","text":"TODO","title":"Architecture"},{"location":"documentation/benchmark-datasets/","text":"Benchmark Datasets TODO","title":"Benchmark Datasets"},{"location":"documentation/benchmark-datasets/#benchmark-datasets","text":"TODO","title":"Benchmark Datasets"},{"location":"documentation/detector-creation/","text":"Detector Creation TODO","title":"Detector Creation"},{"location":"documentation/detector-creation/#detector-creation","text":"TODO","title":"Detector Creation"},{"location":"documentation/ensemble-learning/","text":"Ensemble Learning TODO","title":"Ensemble Learning"},{"location":"documentation/ensemble-learning/#ensemble-learning","text":"TODO","title":"Ensemble Learning"},{"location":"documentation/guide/","text":"Guide This guide should provide you the necessary knowledge to work with OutlierDetection.jl and understand the concepts behind the library design. Note Outlier detection is predominantly an unsupervised learning task , transforming each data point to an outlier score quantifying the level of \"outlierness\". This very general form of output retains all the information provided by a specific algorithm. Key Concepts The key design choice of OutlierDetection.jl is promoting the usage of outlier scores , not labels. The main data type, a Detector , has to implement two methods: fit and score . Detector : A struct defining the hyperparameters for an outlier detection algorithm, just like an estimator in scikit-learn . fit : Learn a Model for a specific detector from input data X and labels y (if supervised), for example the weights of a neural network. score : Using a detector and a learned model, transform unseen data into outlier scores. Transforming the outlier scores to labels is seen as the last step of an outlier detection task. An Evaluator turns scores into probabilities or labels, typically with two classes describing inliers (1) and outliers (-1) . Such an evaluator has to implement a single method: detect . detect : Transform outlier scores to inlier and outlier classes or probabilities. A convention used in OutlierDetection.jl is that higher scores imply higher outlierness . Note A peculiarity of working with outlier scores is the distinction between train scores and test scores . Train scores result from fitting a detector ( fit ), and test scores result from using predicting unseen data ( score ). Classifying an instance as an inlier or outlier always requires a comparison to the train scores. Let's see how the data looks like in a typical outlier detection task. We use the following naming conventions for the data we are working with: Data :: AbstractArray { <: Real } Scores :: AbstractVector { <: Real } Labels :: AbstractVector { <: Integer } Result :: Tuple { Scores , Scores } Because train scores are essential in classification, we often work with tuples of training and test scores and call such a tuple a Result . One last previously unmentioned structure is the Fit result, a struct that bundles the learned model and training scores. Let's now looks how the methods defined by OutlierDetection.jl transform the mentioned data structures. fit ( :: UnsupervisedDetector , :: Data ) :: Fit fit ( :: SupervisedDetector , :: Data , :: Labels ) :: Fit score ( :: Detector , :: Fit , :: Data ) :: Result detect ( :: Evaluator , :: Result ... ) :: Labels One last thing to note is that there are many convenience data transformations implemented. You can use any Tables.jl compatible data source and the framework will make sure that the detectors receive the data in the suitable form. Also, note that detect can work with arbitrarily many results, which is very convenient if you want to combine the results of different detectors. Warning If you are using native Julia arrays AbstractArray{<:Real} as input data, we expect the data to be formatted using the columns-as-observations convention for improved performance with Julia's column-major data. Every other input data will be transposed and converted to an array implicitly. Interoperation with MLJ One of the exciting features of OutlierDetection.jl is it's interoperability with the rest of Julia's machine learning ecosystem. You might want to preprocess your data, cluster it, detect outliers, classify, and so forth. In MLJ, we bind data to a detector using a machine . This data binding allows greater flexibility in later usage; for example, if you use cross-validation for evaluation, the data is split automatically behind the scenes. A machine further enables us to implicitly pass data and learned models to the fit and score methods. From a usage perspective, the main differences are: A Detector is bound to data, either through machine(::UnsupervisedDetector, X) , or machine(::SupervisedDetector, X, y) . fit(::Detector, X, [y]) becomes fit!(machine) score(::Detector, ::Fit, X) becomes transform(machine) detect(::Evaluator, ::Results...) becomes transform(machine(::Evaluator), ::Results...) Take a look at Using MLJ to learn more.","title":"Guide"},{"location":"documentation/guide/#guide","text":"This guide should provide you the necessary knowledge to work with OutlierDetection.jl and understand the concepts behind the library design. Note Outlier detection is predominantly an unsupervised learning task , transforming each data point to an outlier score quantifying the level of \"outlierness\". This very general form of output retains all the information provided by a specific algorithm.","title":"Guide"},{"location":"documentation/guide/#key-concepts","text":"The key design choice of OutlierDetection.jl is promoting the usage of outlier scores , not labels. The main data type, a Detector , has to implement two methods: fit and score . Detector : A struct defining the hyperparameters for an outlier detection algorithm, just like an estimator in scikit-learn . fit : Learn a Model for a specific detector from input data X and labels y (if supervised), for example the weights of a neural network. score : Using a detector and a learned model, transform unseen data into outlier scores. Transforming the outlier scores to labels is seen as the last step of an outlier detection task. An Evaluator turns scores into probabilities or labels, typically with two classes describing inliers (1) and outliers (-1) . Such an evaluator has to implement a single method: detect . detect : Transform outlier scores to inlier and outlier classes or probabilities. A convention used in OutlierDetection.jl is that higher scores imply higher outlierness . Note A peculiarity of working with outlier scores is the distinction between train scores and test scores . Train scores result from fitting a detector ( fit ), and test scores result from using predicting unseen data ( score ). Classifying an instance as an inlier or outlier always requires a comparison to the train scores. Let's see how the data looks like in a typical outlier detection task. We use the following naming conventions for the data we are working with: Data :: AbstractArray { <: Real } Scores :: AbstractVector { <: Real } Labels :: AbstractVector { <: Integer } Result :: Tuple { Scores , Scores } Because train scores are essential in classification, we often work with tuples of training and test scores and call such a tuple a Result . One last previously unmentioned structure is the Fit result, a struct that bundles the learned model and training scores. Let's now looks how the methods defined by OutlierDetection.jl transform the mentioned data structures. fit ( :: UnsupervisedDetector , :: Data ) :: Fit fit ( :: SupervisedDetector , :: Data , :: Labels ) :: Fit score ( :: Detector , :: Fit , :: Data ) :: Result detect ( :: Evaluator , :: Result ... ) :: Labels One last thing to note is that there are many convenience data transformations implemented. You can use any Tables.jl compatible data source and the framework will make sure that the detectors receive the data in the suitable form. Also, note that detect can work with arbitrarily many results, which is very convenient if you want to combine the results of different detectors. Warning If you are using native Julia arrays AbstractArray{<:Real} as input data, we expect the data to be formatted using the columns-as-observations convention for improved performance with Julia's column-major data. Every other input data will be transposed and converted to an array implicitly.","title":"Key Concepts"},{"location":"documentation/guide/#interoperation-with-mlj","text":"One of the exciting features of OutlierDetection.jl is it's interoperability with the rest of Julia's machine learning ecosystem. You might want to preprocess your data, cluster it, detect outliers, classify, and so forth. In MLJ, we bind data to a detector using a machine . This data binding allows greater flexibility in later usage; for example, if you use cross-validation for evaluation, the data is split automatically behind the scenes. A machine further enables us to implicitly pass data and learned models to the fit and score methods. From a usage perspective, the main differences are: A Detector is bound to data, either through machine(::UnsupervisedDetector, X) , or machine(::SupervisedDetector, X, y) . fit(::Detector, X, [y]) becomes fit!(machine) score(::Detector, ::Fit, X) becomes transform(machine) detect(::Evaluator, ::Results...) becomes transform(machine(::Evaluator), ::Results...) Take a look at Using MLJ to learn more.","title":"Interoperation with MLJ"},{"location":"documentation/performance-tips/","text":"Performance Tips All the usual Julia performance tips apply . As always profiling your code is generally a useful way of finding bottlenecks. We provide a basic performance benchmarking toolkit. See benchmark/runbenchmarks.jl for usage instructions.","title":"Performance Tips"},{"location":"documentation/performance-tips/#performance-tips","text":"All the usual Julia performance tips apply . As always profiling your code is generally a useful way of finding bottlenecks. We provide a basic performance benchmarking toolkit. See benchmark/runbenchmarks.jl for usage instructions.","title":"Performance Tips"},{"location":"documentation/using-mlj/","text":"Using MLJ TODO","title":"Using MLJ"},{"location":"documentation/using-mlj/#using-mlj","text":"TODO","title":"Using MLJ"},{"location":"getting-started/contributing/","text":"How to Contribute OutlierDetection.jl is a community-driven project and your help is extremely welcome. If you get stuck, please don't hesitate to chat with us or raise an issue . Take a look at Github's How to Contribute Guide to find out more about what it means to contribute. Note: To avoid duplicating work, it is highly advised that you search through the issue tracker and the PR list . If in doubt about duplicated work, or if you want to work on a non-trivial feature, it\u2019s recommended to first open an issue in the issue tracker to get some feedbacks from core developers. Areas of contribution We value all kinds of contributions - not just code. The following table gives an overview of key contribution areas. Area Description Documentation Improve or add docstrings, glossary terms, the user guide, and the example notebooks Testing Report bugs, improve or add unit tests, conduct field testing on real-world data sets Code Improve or add functionality, fix bugs Mentoring Onboarding and mentoring of new contributors Outreach Organize talks, tutorials or workshops, write blog posts Maintenance Manage and review issues/pull requests API design Design interfaces for detectors and other functionality Reporting bugs We use GitHub issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish to see a feature implemented. It is recommended to check that your issue complies with the following rules before submitting: Verify that your issue is not being currently addressed by other issues or pull requests . Please ensure all code snippets and error messages are formatted in appropriate code blocks. See Creating and highlighting code blocks . Please be specific about what detectors and/or functions are involved and the shape of the data, as appropriate; please include a reproducible code snippet or link to a gist . If an exception is raised, please provide the traceback. The contribution workflow The preferred workflow for contributing to OutlierDetection's repository is to fork the main repository on GitHub, clone, and develop on a new branch. Fork the project repository by clicking on the \\'Fork\\' button near the top right of the page. This creates a copy of the code under your GitHub user account. For more details on how to fork a repository see this guide . Clone your fork of the OutlierDetection.jl repo from your GitHub account to your local disk: git clone git@github.com:USERNAME/OutlierDetection.jl.git cd OutlierDetection.jl Configure and link the remote for your fork to the upstream repository: git remote -v git remote add upstream https://github.com/davnn/OutlierDetection.git Verify the new upstream repository you\\'ve specified for your fork: git remote -v > origin https://github.com/USERNAME/YOUR_FORK.git ( fetch ) > origin https://github.com/YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( fetch ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( push ) Sync the main branch of your fork with the upstream repository: git fetch upstream git checkout main --track origin/main git merge upstream/main Create a new feature branch from the main branch to hold your changes: git checkout main git checkout -b <my-feature-branch> Always use a feature branch. It\\'s good practice to never work on the main branch! Name the feature branch after your contribution. Develop your contribution on your feature branch. Add changed files using git add and then git commit files to record your changes in Git: git add <modified_files> git commit When finished, push the changes to your GitHub account with: git push --set-upstream origin my-feature-branch Follow these instructions to create a pull request from your fork. If your work is still work in progress, you can open a draft pull request. We recommend to open a pull request early, so that other contributors become aware of your work and can give you feedback early on. To add more changes, simply repeat steps 7 - 8. Pull requests are updated automatically if you push new changes to the same branch. If any of the above seems like magic to you, please look up the Git documentation on the web. If you get stuck, feel free to chat with us . Continuous integration We use continuous integration services on GitHub to automatically check if new pull requests do not break anything on all the Julia versions we support. The main quality control measures right now are unit testing and test coverage . In the future we additionally want to check code style and formatting. Unit testing We use Julia's built-in Unit Testing . The tests can be found in the test folder. To check if your code passes all tests make sure that you have the OutlierDetection environment activated and run ] test from the Julia console. Test coverage We use the Coverage.jl package and codecov to measure and compare test coverage of our code. API design We follow the general design approach chosen by MLJ, which is described in the paper \"Designing Machine Learning Toolboxes: Concepts, Principles and Patterns\" . Additionally, we are always looking for feedback and improvement suggestions! Documentation We use Documenter.jl and mkdocs to build and deploy our online documention. The source files used to generate the online documentation can be found in docs/src/ . For example, the main configuration file for mkdocs is mkdocs.yml and the main page is index.md . To add new pages, you need to add a new .md file and include it in the mkdocs.yml file. To build the documentation locally, you need to navigate to docs/ and Build the markdown files with Documenter.jl: julia --project make.jl To build the website using the markdown files, run: mkdocs build # optionally run `mkdocs serve` to build and serve locally You can find the generated files in the OutlierDetection.jl/docs/site/ folder. To view the website, open OutlierDetection.jl/docs/site/index.html with your preferred web browser or use mkdocs serve to start a local documentation server. Coding style We use DocumentFormat.jl as a code formatter. Additionally, we use a maximum line length of 120 characters. Acknowledging contributions We follow the all-contributors specification and recognise various types of contributions. Take a look at our past and current contributors ! If you are a new contributor, please make sure we add you to our list of contributors. All contributions are recorded in .all-contributorsrc .","title":"Contributing"},{"location":"getting-started/contributing/#how-to-contribute","text":"OutlierDetection.jl is a community-driven project and your help is extremely welcome. If you get stuck, please don't hesitate to chat with us or raise an issue . Take a look at Github's How to Contribute Guide to find out more about what it means to contribute. Note: To avoid duplicating work, it is highly advised that you search through the issue tracker and the PR list . If in doubt about duplicated work, or if you want to work on a non-trivial feature, it\u2019s recommended to first open an issue in the issue tracker to get some feedbacks from core developers.","title":"How to Contribute"},{"location":"getting-started/contributing/#areas-of-contribution","text":"We value all kinds of contributions - not just code. The following table gives an overview of key contribution areas. Area Description Documentation Improve or add docstrings, glossary terms, the user guide, and the example notebooks Testing Report bugs, improve or add unit tests, conduct field testing on real-world data sets Code Improve or add functionality, fix bugs Mentoring Onboarding and mentoring of new contributors Outreach Organize talks, tutorials or workshops, write blog posts Maintenance Manage and review issues/pull requests API design Design interfaces for detectors and other functionality","title":"Areas of contribution"},{"location":"getting-started/contributing/#reporting-bugs","text":"We use GitHub issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish to see a feature implemented. It is recommended to check that your issue complies with the following rules before submitting: Verify that your issue is not being currently addressed by other issues or pull requests . Please ensure all code snippets and error messages are formatted in appropriate code blocks. See Creating and highlighting code blocks . Please be specific about what detectors and/or functions are involved and the shape of the data, as appropriate; please include a reproducible code snippet or link to a gist . If an exception is raised, please provide the traceback.","title":"Reporting bugs"},{"location":"getting-started/contributing/#the-contribution-workflow","text":"The preferred workflow for contributing to OutlierDetection's repository is to fork the main repository on GitHub, clone, and develop on a new branch. Fork the project repository by clicking on the \\'Fork\\' button near the top right of the page. This creates a copy of the code under your GitHub user account. For more details on how to fork a repository see this guide . Clone your fork of the OutlierDetection.jl repo from your GitHub account to your local disk: git clone git@github.com:USERNAME/OutlierDetection.jl.git cd OutlierDetection.jl Configure and link the remote for your fork to the upstream repository: git remote -v git remote add upstream https://github.com/davnn/OutlierDetection.git Verify the new upstream repository you\\'ve specified for your fork: git remote -v > origin https://github.com/USERNAME/YOUR_FORK.git ( fetch ) > origin https://github.com/YOUR_USERNAME/YOUR_FORK.git ( push ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( fetch ) > upstream https://github.com/davnn/OutlierDetection.jl.git ( push ) Sync the main branch of your fork with the upstream repository: git fetch upstream git checkout main --track origin/main git merge upstream/main Create a new feature branch from the main branch to hold your changes: git checkout main git checkout -b <my-feature-branch> Always use a feature branch. It\\'s good practice to never work on the main branch! Name the feature branch after your contribution. Develop your contribution on your feature branch. Add changed files using git add and then git commit files to record your changes in Git: git add <modified_files> git commit When finished, push the changes to your GitHub account with: git push --set-upstream origin my-feature-branch Follow these instructions to create a pull request from your fork. If your work is still work in progress, you can open a draft pull request. We recommend to open a pull request early, so that other contributors become aware of your work and can give you feedback early on. To add more changes, simply repeat steps 7 - 8. Pull requests are updated automatically if you push new changes to the same branch. If any of the above seems like magic to you, please look up the Git documentation on the web. If you get stuck, feel free to chat with us .","title":"The contribution workflow"},{"location":"getting-started/contributing/#continuous-integration","text":"We use continuous integration services on GitHub to automatically check if new pull requests do not break anything on all the Julia versions we support. The main quality control measures right now are unit testing and test coverage . In the future we additionally want to check code style and formatting.","title":"Continuous integration"},{"location":"getting-started/contributing/#unit-testing","text":"We use Julia's built-in Unit Testing . The tests can be found in the test folder. To check if your code passes all tests make sure that you have the OutlierDetection environment activated and run ] test from the Julia console.","title":"Unit testing"},{"location":"getting-started/contributing/#test-coverage","text":"We use the Coverage.jl package and codecov to measure and compare test coverage of our code.","title":"Test coverage"},{"location":"getting-started/contributing/#api-design","text":"We follow the general design approach chosen by MLJ, which is described in the paper \"Designing Machine Learning Toolboxes: Concepts, Principles and Patterns\" . Additionally, we are always looking for feedback and improvement suggestions!","title":"API design"},{"location":"getting-started/contributing/#documentation","text":"We use Documenter.jl and mkdocs to build and deploy our online documention. The source files used to generate the online documentation can be found in docs/src/ . For example, the main configuration file for mkdocs is mkdocs.yml and the main page is index.md . To add new pages, you need to add a new .md file and include it in the mkdocs.yml file. To build the documentation locally, you need to navigate to docs/ and Build the markdown files with Documenter.jl: julia --project make.jl To build the website using the markdown files, run: mkdocs build # optionally run `mkdocs serve` to build and serve locally You can find the generated files in the OutlierDetection.jl/docs/site/ folder. To view the website, open OutlierDetection.jl/docs/site/index.html with your preferred web browser or use mkdocs serve to start a local documentation server.","title":"Documentation"},{"location":"getting-started/contributing/#coding-style","text":"We use DocumentFormat.jl as a code formatter. Additionally, we use a maximum line length of 120 characters.","title":"Coding style"},{"location":"getting-started/contributing/#acknowledging-contributions","text":"We follow the all-contributors specification and recognise various types of contributions. Take a look at our past and current contributors ! If you are a new contributor, please make sure we add you to our list of contributors. All contributions are recorded in .all-contributorsrc .","title":"Acknowledging contributions"},{"location":"getting-started/example/","text":"Example This example demonstrates using the raw OutlierDetection API to determine the outlierness of instances in the Thyroid Disease Dataset , which is part of the ODDS collection . We use OutlierDetectionData.jl to download and read the dataset. Note that the raw API uses the columns-as-observations convention for improved performance. Import OutlierDetection and OutlierDetectionData using OutlierDetection using OutlierDetectionData : ODDS Download and read the \"thyroid\" dataset from the ODDS collection. X , y = ODDS . load ( \"thyroid\" ) Create indices to split the data into 50% training and test data. n_train = Int(length(y) * 0.5) train, test = eachindex(y)[1:n_train], eachindex(y)[n_train+1:end] Initialize an unsupervised KNN Detector with k=10 neighbors. detector = KNN ( k = 10 ) Learn a model from the data X . fitresult = fit ( detector , X [ train , : ] ' ) Evaluate the resulting training data scores (stored in the fit result). roc_auc ( y [ train ], fitresult . scores ) Calculate the outlier scores for our test data. Note that we always return both the train and test scores because later used evaluators typically choose a threshold based on the train scores. scores_train , scores_test = score ( detector , fitresult , X [ test , : ] ' ) Evaluate the resulting test scores with the given labels. roc_auc ( y [ test ], scores_test ) You can easily convert the obtained scores into inlier ( 1 ), and outlier ( -1 ) labels using an Evaluator , in this case, Class . detect ( Class (), scores_train , scores_test ) Using MLJ Typically, you do not use the OutlierDetection API, but instead, use MLJ to interface with OutlierDetection.jl . The main difference between the raw API and MLJ is, besides method naming differences, the introduction of a machine . In the raw API, we explicitly pass data and results to fit and score calls. Machines allow us to hide that complexity by implicitly passing data and results. Given that you have already imported OutlierDetection and loaded X and y as described before, import MLJ . using MLJ # or MLJBase Create a pipeline consisting of a detector and classifier. pipe = @pipeline KNN ( k = 10 ) Class () Bind the pipeline to data to create a machine. mach = machine ( pipe , X ) Fit the machine to learn from the training input data. fit! ( mach , rows = train ) Predict the labels for the test data with the learned machine. transform ( mach , rows = test ) Learn more To learn more about the concepts in OutlierDetection.jl , check out the guide !","title":"Example"},{"location":"getting-started/example/#example","text":"This example demonstrates using the raw OutlierDetection API to determine the outlierness of instances in the Thyroid Disease Dataset , which is part of the ODDS collection . We use OutlierDetectionData.jl to download and read the dataset. Note that the raw API uses the columns-as-observations convention for improved performance. Import OutlierDetection and OutlierDetectionData using OutlierDetection using OutlierDetectionData : ODDS Download and read the \"thyroid\" dataset from the ODDS collection. X , y = ODDS . load ( \"thyroid\" ) Create indices to split the data into 50% training and test data. n_train = Int(length(y) * 0.5) train, test = eachindex(y)[1:n_train], eachindex(y)[n_train+1:end] Initialize an unsupervised KNN Detector with k=10 neighbors. detector = KNN ( k = 10 ) Learn a model from the data X . fitresult = fit ( detector , X [ train , : ] ' ) Evaluate the resulting training data scores (stored in the fit result). roc_auc ( y [ train ], fitresult . scores ) Calculate the outlier scores for our test data. Note that we always return both the train and test scores because later used evaluators typically choose a threshold based on the train scores. scores_train , scores_test = score ( detector , fitresult , X [ test , : ] ' ) Evaluate the resulting test scores with the given labels. roc_auc ( y [ test ], scores_test ) You can easily convert the obtained scores into inlier ( 1 ), and outlier ( -1 ) labels using an Evaluator , in this case, Class . detect ( Class (), scores_train , scores_test )","title":"Example"},{"location":"getting-started/example/#using-mlj","text":"Typically, you do not use the OutlierDetection API, but instead, use MLJ to interface with OutlierDetection.jl . The main difference between the raw API and MLJ is, besides method naming differences, the introduction of a machine . In the raw API, we explicitly pass data and results to fit and score calls. Machines allow us to hide that complexity by implicitly passing data and results. Given that you have already imported OutlierDetection and loaded X and y as described before, import MLJ . using MLJ # or MLJBase Create a pipeline consisting of a detector and classifier. pipe = @pipeline KNN ( k = 10 ) Class () Bind the pipeline to data to create a machine. mach = machine ( pipe , X ) Fit the machine to learn from the training input data. fit! ( mach , rows = train ) Predict the labels for the test data with the learned machine. transform ( mach , rows = test )","title":"Using MLJ"},{"location":"getting-started/example/#learn-more","text":"To learn more about the concepts in OutlierDetection.jl , check out the guide !","title":"Learn more"},{"location":"getting-started/installation/","text":"Installation It is recommended to use Pkg.jl for installation. Please make sure that you are using a compatible version of Julia. A list of compatible versions can be found our CI pipeline . Follow the command below to install the latest official release or use ] add OutlierDetection in the Julia REPL. import Pkg ; Pkg . add ( \"OutlierDetection\" ) A specific version can be installed by appending a version after a @ symbol, e.g. OutlierDetection@v0.1 . Additionally, you can directly install specific branches or commits by appending a # symbol and the corresponding branch name or commit SHA, e.g. OutlierDetection#master . If you would like to modify the package locally, you can use Pkg.develop(OutlierDetection) or ] dev OutlierDetection in the Julia REPL. This fetches a full clone of the package to ~/.julia/dev/ (the path can be changed by setting the environment variable JULIA_PKG_DEVDIR ).","title":"Installation"},{"location":"getting-started/installation/#installation","text":"It is recommended to use Pkg.jl for installation. Please make sure that you are using a compatible version of Julia. A list of compatible versions can be found our CI pipeline . Follow the command below to install the latest official release or use ] add OutlierDetection in the Julia REPL. import Pkg ; Pkg . add ( \"OutlierDetection\" ) A specific version can be installed by appending a version after a @ symbol, e.g. OutlierDetection@v0.1 . Additionally, you can directly install specific branches or commits by appending a # symbol and the corresponding branch name or commit SHA, e.g. OutlierDetection#master . If you would like to modify the package locally, you can use Pkg.develop(OutlierDetection) or ] dev OutlierDetection in the Julia REPL. This fetches a full clone of the package to ~/.julia/dev/ (the path can be changed by setting the environment variable JULIA_PKG_DEVDIR ).","title":"Installation"}]}